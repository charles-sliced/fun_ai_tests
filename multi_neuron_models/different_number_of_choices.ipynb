{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2897db07-327b-4d87-9d4e-34a843976179",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "\n",
      "Running experiment with 1 computation neurons per selection neuron\n",
      "\n",
      "Epoch 1: Train Loss: 0.8221, Test Loss: 0.3437, Test Accuracy: 90.23%\n",
      "Layer 1 Entropy: 0.0000, Layer 2 Entropy: 0.0000\n",
      "Layer 1 Selection Ratios: [1.]\n",
      "Layer 2 Selection Ratios: [1.]\n",
      "\n",
      "Epoch 2: Train Loss: 0.3148, Test Loss: 0.2740, Test Accuracy: 92.04%\n",
      "Layer 1 Entropy: 0.0000, Layer 2 Entropy: 0.0000\n",
      "Layer 1 Selection Ratios: [1.]\n",
      "Layer 2 Selection Ratios: [1.]\n",
      "\n",
      "Epoch 3: Train Loss: 0.2618, Test Loss: 0.2337, Test Accuracy: 93.26%\n",
      "Layer 1 Entropy: 0.0000, Layer 2 Entropy: 0.0000\n",
      "Layer 1 Selection Ratios: [1.]\n",
      "Layer 2 Selection Ratios: [1.]\n",
      "\n",
      "Epoch 4: Train Loss: 0.2266, Test Loss: 0.2067, Test Accuracy: 94.05%\n",
      "Layer 1 Entropy: 0.0000, Layer 2 Entropy: 0.0000\n",
      "Layer 1 Selection Ratios: [1.]\n",
      "Layer 2 Selection Ratios: [1.]\n",
      "\n",
      "Epoch 5: Train Loss: 0.2005, Test Loss: 0.1858, Test Accuracy: 94.67%\n",
      "Layer 1 Entropy: 0.0000, Layer 2 Entropy: 0.0000\n",
      "Layer 1 Selection Ratios: [1.]\n",
      "Layer 2 Selection Ratios: [1.]\n",
      "\n",
      "\n",
      "Running experiment with 2 computation neurons per selection neuron\n",
      "\n",
      "Epoch 1: Train Loss: 0.8068, Test Loss: 0.3230, Test Accuracy: 90.56%\n",
      "Layer 1 Entropy: 0.6930, Layer 2 Entropy: 0.6926\n",
      "Layer 1 Selection Ratios: [0.49246823 0.50753177]\n",
      "Layer 2 Selection Ratios: [0.51678203 0.48321797]\n",
      "\n",
      "Epoch 2: Train Loss: 0.2827, Test Loss: 0.2370, Test Accuracy: 93.06%\n",
      "Layer 1 Entropy: 0.6930, Layer 2 Entropy: 0.6927\n",
      "Layer 1 Selection Ratios: [0.50705964 0.49294036]\n",
      "Layer 2 Selection Ratios: [0.51557135 0.48442865]\n",
      "\n",
      "Epoch 3: Train Loss: 0.2162, Test Loss: 0.1898, Test Accuracy: 94.33%\n",
      "Layer 1 Entropy: 0.6926, Layer 2 Entropy: 0.6930\n",
      "Layer 1 Selection Ratios: [0.51610156 0.48389844]\n",
      "Layer 2 Selection Ratios: [0.50723906 0.49276094]\n",
      "\n",
      "Epoch 4: Train Loss: 0.1779, Test Loss: 0.1636, Test Accuracy: 95.01%\n",
      "Layer 1 Entropy: 0.6923, Layer 2 Entropy: 0.6931\n",
      "Layer 1 Selection Ratios: [0.52079609 0.47920391]\n",
      "Layer 2 Selection Ratios: [0.50024297 0.49975703]\n",
      "\n",
      "Epoch 5: Train Loss: 0.1534, Test Loss: 0.1437, Test Accuracy: 95.77%\n",
      "Layer 1 Entropy: 0.6919, Layer 2 Entropy: 0.6931\n",
      "Layer 1 Selection Ratios: [0.5254612 0.4745388]\n",
      "Layer 2 Selection Ratios: [0.49784115 0.50215885]\n",
      "\n",
      "\n",
      "Running experiment with 3 computation neurons per selection neuron\n",
      "\n",
      "Epoch 1: Train Loss: 0.8468, Test Loss: 0.3223, Test Accuracy: 90.41%\n",
      "Layer 1 Entropy: 1.0959, Layer 2 Entropy: 1.0968\n",
      "Layer 1 Selection Ratios: [0.32163099 0.36772865 0.31064036]\n",
      "Layer 2 Selection Ratios: [0.34295651 0.30574115 0.35130234]\n",
      "\n",
      "Epoch 2: Train Loss: 0.2752, Test Loss: 0.2256, Test Accuracy: 93.49%\n",
      "Layer 1 Entropy: 1.0968, Layer 2 Entropy: 1.0973\n",
      "Layer 1 Selection Ratios: [0.32988333 0.35951979 0.31059687]\n",
      "Layer 2 Selection Ratios: [0.33923177 0.31011302 0.35065521]\n",
      "\n",
      "Epoch 3: Train Loss: 0.2106, Test Loss: 0.1866, Test Accuracy: 94.49%\n",
      "Layer 1 Entropy: 1.0975, Layer 2 Entropy: 1.0971\n",
      "Layer 1 Selection Ratios: [0.33469922 0.35161432 0.31368646]\n",
      "Layer 2 Selection Ratios: [0.33443516 0.31008958 0.35547526]\n",
      "\n",
      "Epoch 4: Train Loss: 0.1754, Test Loss: 0.1631, Test Accuracy: 95.28%\n",
      "Layer 1 Entropy: 1.0977, Layer 2 Entropy: 1.0972\n",
      "Layer 1 Selection Ratios: [0.33707995 0.34817969 0.31474036]\n",
      "Layer 2 Selection Ratios: [0.33301406 0.31173255 0.35525339]\n",
      "\n",
      "Epoch 5: Train Loss: 0.1515, Test Loss: 0.1470, Test Accuracy: 95.55%\n",
      "Layer 1 Entropy: 1.0978, Layer 2 Entropy: 1.0972\n",
      "Layer 1 Selection Ratios: [0.338375   0.34644974 0.31517526]\n",
      "Layer 2 Selection Ratios: [0.33077135 0.3127362  0.35649245]\n",
      "\n",
      "\n",
      "Running experiment with 4 computation neurons per selection neuron\n",
      "\n",
      "Epoch 1: Train Loss: 0.9219, Test Loss: 0.3400, Test Accuracy: 89.95%\n",
      "Layer 1 Entropy: 1.3814, Layer 2 Entropy: 1.3815\n",
      "Layer 1 Selection Ratios: [0.29255339 0.23387266 0.24401042 0.22956354]\n",
      "Layer 2 Selection Ratios: [0.27158255 0.26000078 0.20906563 0.25935104]\n",
      "\n",
      "Epoch 2: Train Loss: 0.2840, Test Loss: 0.2318, Test Accuracy: 92.97%\n",
      "Layer 1 Entropy: 1.3828, Layer 2 Entropy: 1.3802\n",
      "Layer 1 Selection Ratios: [0.2861974  0.24305625 0.23902813 0.23171823]\n",
      "Layer 2 Selection Ratios: [0.26763854 0.27245755 0.20416667 0.25573724]\n",
      "\n",
      "Epoch 3: Train Loss: 0.2145, Test Loss: 0.1909, Test Accuracy: 94.34%\n",
      "Layer 1 Entropy: 1.3827, Layer 2 Entropy: 1.3817\n",
      "Layer 1 Selection Ratios: [0.28343203 0.25323698 0.22994036 0.23339062]\n",
      "Layer 2 Selection Ratios: [0.2680862  0.2673013  0.21034375 0.25426875]\n",
      "\n",
      "Epoch 4: Train Loss: 0.1781, Test Loss: 0.1653, Test Accuracy: 95.07%\n",
      "Layer 1 Entropy: 1.3822, Layer 2 Entropy: 1.3822\n",
      "Layer 1 Selection Ratios: [0.28403177 0.2565599  0.22586615 0.23354219]\n",
      "Layer 2 Selection Ratios: [0.26908385 0.26163333 0.21216484 0.25711797]\n",
      "\n",
      "Epoch 5: Train Loss: 0.1513, Test Loss: 0.1535, Test Accuracy: 95.38%\n",
      "Layer 1 Entropy: 1.3820, Layer 2 Entropy: 1.3824\n",
      "Layer 1 Selection Ratios: [0.28402005 0.25841797 0.22431094 0.23325104]\n",
      "Layer 2 Selection Ratios: [0.26875339 0.25790417 0.21295938 0.26038307]\n",
      "\n",
      "\n",
      "Running experiment with 5 computation neurons per selection neuron\n",
      "\n",
      "Epoch 1: Train Loss: 0.9775, Test Loss: 0.3517, Test Accuracy: 89.60%\n",
      "Layer 1 Entropy: 1.6058, Layer 2 Entropy: 1.6028\n",
      "Layer 1 Selection Ratios: [0.20093984 0.17425885 0.22764583 0.19383359 0.20332188]\n",
      "Layer 2 Selection Ratios: [0.22752969 0.19715677 0.17490729 0.17490677 0.22549948]\n",
      "\n",
      "Epoch 2: Train Loss: 0.2909, Test Loss: 0.2378, Test Accuracy: 92.89%\n",
      "Layer 1 Entropy: 1.6060, Layer 2 Entropy: 1.5961\n",
      "Layer 1 Selection Ratios: [0.2004474  0.17168047 0.22212682 0.19814193 0.20760339]\n",
      "Layer 2 Selection Ratios: [0.2283362  0.20183958 0.16592031 0.16115937 0.24274453]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Check for GPU availability\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f'Using device: {device}')\n",
    "\n",
    "# Function to compute entropy\n",
    "def compute_entropy(p, eps=1e-12):\n",
    "    p = np.clip(p, eps, 1 - eps)\n",
    "    return -np.sum(p * np.log(p), axis=-1)\n",
    "\n",
    "# Prepare the MNIST dataset and load it into GPU memory\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.1307,), (0.3081,))\n",
    "])\n",
    "\n",
    "# Load the entire training dataset\n",
    "train_dataset_full = datasets.MNIST(root='./data', train=True,\n",
    "                                    transform=transform, download=True)\n",
    "test_dataset = datasets.MNIST(root='./data', train=False,\n",
    "                              transform=transform)\n",
    "\n",
    "# Move training data to GPU\n",
    "train_data = train_dataset_full.data.float().div(255).unsqueeze(1).to(device)  # Shape: [60000, 1, 28, 28]\n",
    "train_data = train_data.sub_(0.1307).div_(0.3081)  # Normalize\n",
    "train_targets = train_dataset_full.targets.to(device)  # Shape: [60000]\n",
    "\n",
    "# Similarly for test data\n",
    "test_data = test_dataset.data.float().div(255).unsqueeze(1).to(device)  # Shape: [10000, 1, 28, 28]\n",
    "test_data = test_data.sub_(0.1307).div_(0.3081)  # Normalize\n",
    "test_targets = test_dataset.targets.to(device)  # Shape: [10000]\n",
    "\n",
    "# Function to create DataLoader from in-memory data\n",
    "def create_data_loader(data, targets, batch_size):\n",
    "    dataset = torch.utils.data.TensorDataset(data, targets)\n",
    "    loader = torch.utils.data.DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "    return loader\n",
    "\n",
    "# Define the custom neuron module with variable computation neurons\n",
    "class CustomNeuron(nn.Module):\n",
    "    def __init__(self, input_size, num_comp_neurons):\n",
    "        super(CustomNeuron, self).__init__()\n",
    "        self.num_comp_neurons = num_comp_neurons\n",
    "        # Selection neuron outputs N logits\n",
    "        self.selection_layer = nn.Linear(input_size, num_comp_neurons)\n",
    "        # Computation neurons (linear layers)\n",
    "        self.comp_layers = nn.ModuleList([nn.Linear(input_size, 1) for _ in range(num_comp_neurons)])\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # Selection logits\n",
    "        selection_logits = self.selection_layer(x)  # Shape: [batch_size, num_comp_neurons]\n",
    "        # Softmax to get selection probabilities\n",
    "        selection_probs = F.softmax(selection_logits, dim=1)  # Shape: [batch_size, num_comp_neurons]\n",
    "        \n",
    "        # Hard selection using argmax\n",
    "        with torch.no_grad():\n",
    "            selected_idx = torch.argmax(selection_probs, dim=1)  # Shape: [batch_size]\n",
    "            selected_mask_hard = F.one_hot(selected_idx, num_classes=self.num_comp_neurons).float()  # Shape: [batch_size, num_comp_neurons]\n",
    "        # Straight-through estimator\n",
    "        selected_mask = selected_mask_hard - selection_probs.detach() + selection_probs  # Shape: [batch_size, num_comp_neurons]\n",
    "        \n",
    "        # Compute outputs from all computation neurons\n",
    "        comp_outputs = [torch.relu(comp_layer(x)) for comp_layer in self.comp_layers]  # List of [batch_size, 1]\n",
    "        comp_outputs = torch.cat(comp_outputs, dim=1)  # Shape: [batch_size, num_comp_neurons]\n",
    "        \n",
    "        # Combine outputs using selected_mask\n",
    "        comp_output = torch.sum(comp_outputs * selected_mask, dim=1, keepdim=True)  # Shape: [batch_size, 1]\n",
    "        \n",
    "        # Store selected_mask_hard for analysis\n",
    "        self.selected_mask_hard = selected_mask_hard.detach()\n",
    "        \n",
    "        return comp_output\n",
    "\n",
    "# Define the custom layer module\n",
    "class CustomLayer(nn.Module):\n",
    "    def __init__(self, input_size, output_size, num_comp_neurons):\n",
    "        super(CustomLayer, self).__init__()\n",
    "        self.neurons = nn.ModuleList([CustomNeuron(input_size, num_comp_neurons) for _ in range(output_size)])\n",
    "    \n",
    "    def forward(self, x):\n",
    "        outputs = []\n",
    "        selected_masks_hard = []\n",
    "        for neuron in self.neurons:\n",
    "            output = neuron(x)  # Each output is [batch_size, 1]\n",
    "            outputs.append(output)\n",
    "            selected_masks_hard.append(neuron.selected_mask_hard)  # Each is [batch_size, num_comp_neurons]\n",
    "        x_out = torch.cat(outputs, dim=1)  # Shape: [batch_size, output_size]\n",
    "        selected_masks_hard = torch.stack(selected_masks_hard, dim=1)  # Shape: [batch_size, output_size, num_comp_neurons]\n",
    "        self.selected_masks_hard = selected_masks_hard  # For analysis\n",
    "        return x_out\n",
    "\n",
    "# Define the custom network\n",
    "class CustomNet(nn.Module):\n",
    "    def __init__(self, num_comp_neurons):\n",
    "        super(CustomNet, self).__init__()\n",
    "        self.layer1 = CustomLayer(28*28, 64, num_comp_neurons)  # Input layer\n",
    "        self.layer2 = CustomLayer(64, 64, num_comp_neurons)     # Hidden layer\n",
    "        self.fc = nn.Linear(64, 10)           # Output layer\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = x.view(-1, 28*28)  # Flatten the image\n",
    "        \n",
    "        # Pass through the first custom layer\n",
    "        x = self.layer1(x)\n",
    "        selected_masks_layer1 = self.layer1.selected_masks_hard  # Shape: [batch_size, 64, num_comp_neurons]\n",
    "        \n",
    "        # Pass through the second custom layer\n",
    "        x = self.layer2(x)\n",
    "        selected_masks_layer2 = self.layer2.selected_masks_hard  # Shape: [batch_size, 64, num_comp_neurons]\n",
    "        \n",
    "        # Output layer\n",
    "        x = self.fc(x)\n",
    "        \n",
    "        # Store selected masks for analysis\n",
    "        self.selected_masks_layer1 = selected_masks_layer1\n",
    "        self.selected_masks_layer2 = selected_masks_layer2\n",
    "        \n",
    "        return x\n",
    "\n",
    "# Number of computation neurons to test\n",
    "comp_neurons_list = [1, 2, 3, 4, 5]\n",
    "num_epochs = 5\n",
    "\n",
    "# Dictionaries to store metrics for each run\n",
    "all_train_losses = {}\n",
    "all_test_losses = {}\n",
    "all_test_accuracies = {}\n",
    "all_entropy_layer1 = {}\n",
    "all_entropy_layer2 = {}\n",
    "all_selection_ratios_layer1 = {}\n",
    "all_selection_ratios_layer2 = {}\n",
    "\n",
    "for num_comp_neurons in comp_neurons_list:\n",
    "    print(f'\\nRunning experiment with {num_comp_neurons} computation neurons per selection neuron\\n')\n",
    "    # Initialize the network, criterion, and optimizer\n",
    "    model = CustomNet(num_comp_neurons).to(device)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.SGD(model.parameters(), lr=0.01)\n",
    "    \n",
    "    # Create DataLoaders\n",
    "    batch_size = 64\n",
    "    train_loader = create_data_loader(train_data, train_targets, batch_size)\n",
    "    test_loader = create_data_loader(test_data, test_targets, 1000)\n",
    "    \n",
    "    # Initialize lists to store metrics\n",
    "    epoch_train_losses = []\n",
    "    epoch_test_losses = []\n",
    "    epoch_test_accuracies = []\n",
    "    epoch_entropy_layer1 = []\n",
    "    epoch_entropy_layer2 = []\n",
    "    epoch_selection_ratios_layer1 = []\n",
    "    epoch_selection_ratios_layer2 = []\n",
    "    \n",
    "    # Training loop\n",
    "    for epoch in range(1, num_epochs + 1):\n",
    "        model.train()\n",
    "        train_loss = 0\n",
    "        # Initialize selection counts for entropy calculation\n",
    "        selection_counts_layer1 = torch.zeros(model.layer1.neurons.__len__(), num_comp_neurons).to(device)  # Shape: [64, num_comp_neurons]\n",
    "        selection_counts_layer2 = torch.zeros(model.layer2.neurons.__len__(), num_comp_neurons).to(device)  # Shape: [64, num_comp_neurons]\n",
    "        total_selections_layer1 = 0\n",
    "        total_selections_layer2 = 0\n",
    "        \n",
    "        for batch_idx, (data, target) in enumerate(train_loader):\n",
    "            data, target = data.to(device), target.to(device)  # Data is already on device\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            output = model(data)\n",
    "            loss = criterion(output, target)\n",
    "            \n",
    "            loss.backward()\n",
    "            \n",
    "            optimizer.step()\n",
    "            \n",
    "            # Accumulate training loss\n",
    "            train_loss += loss.item() * data.size(0)\n",
    "            \n",
    "            # Collect selection counts\n",
    "            selected_masks_layer1 = model.selected_masks_layer1  # Shape: [batch_size, 64, num_comp_neurons]\n",
    "            selected_masks_layer2 = model.selected_masks_layer2  # Shape: [batch_size, 64, num_comp_neurons]\n",
    "            \n",
    "            # Sum over batch dimension\n",
    "            selection_counts_layer1 += selected_masks_layer1.sum(dim=0)  # Shape: [64, num_comp_neurons]\n",
    "            total_selections_layer1 += selected_masks_layer1.shape[0]  # Number of samples in batch\n",
    "            \n",
    "            selection_counts_layer2 += selected_masks_layer2.sum(dim=0)  # Shape: [64, num_comp_neurons]\n",
    "            total_selections_layer2 += selected_masks_layer2.shape[0]  # Number of samples in batch\n",
    "        \n",
    "        # Compute average loss for the epoch\n",
    "        train_loss /= len(train_loader.dataset)\n",
    "        epoch_train_losses.append(train_loss)\n",
    "        \n",
    "        # Compute selection ratios\n",
    "        selection_ratios_layer1 = selection_counts_layer1.sum(dim=0).cpu().numpy() / (total_selections_layer1 * model.layer1.neurons.__len__())\n",
    "        selection_ratios_layer2 = selection_counts_layer2.sum(dim=0).cpu().numpy() / (total_selections_layer2 * model.layer2.neurons.__len__())\n",
    "        epoch_selection_ratios_layer1.append(selection_ratios_layer1)\n",
    "        epoch_selection_ratios_layer2.append(selection_ratios_layer2)\n",
    "        \n",
    "        # Compute entropy\n",
    "        entropy_layer1 = compute_entropy(selection_ratios_layer1)\n",
    "        entropy_layer2 = compute_entropy(selection_ratios_layer2)\n",
    "        epoch_entropy_layer1.append(entropy_layer1)\n",
    "        epoch_entropy_layer2.append(entropy_layer2)\n",
    "        \n",
    "        # Testing loop\n",
    "        model.eval()\n",
    "        test_loss = 0\n",
    "        correct = 0\n",
    "        with torch.no_grad():\n",
    "            for data, target in test_loader:\n",
    "                data, target = data.to(device), target.to(device)\n",
    "                output = model(data)\n",
    "                test_loss += criterion(output, target).item() * data.size(0)\n",
    "                pred = output.argmax(dim=1, keepdim=True)\n",
    "                correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "        test_loss /= len(test_loader.dataset)\n",
    "        accuracy = 100. * correct / len(test_loader.dataset)\n",
    "        epoch_test_losses.append(test_loss)\n",
    "        epoch_test_accuracies.append(accuracy)\n",
    "        \n",
    "        # Print end-of-epoch results\n",
    "        print(f'Epoch {epoch}: Train Loss: {train_loss:.4f}, Test Loss: {test_loss:.4f}, Test Accuracy: {accuracy:.2f}%')\n",
    "        print(f'Layer 1 Entropy: {np.sum(entropy_layer1):.4f}, Layer 2 Entropy: {np.sum(entropy_layer2):.4f}')\n",
    "        print(f'Layer 1 Selection Ratios: {selection_ratios_layer1}')\n",
    "        print(f'Layer 2 Selection Ratios: {selection_ratios_layer2}\\n')\n",
    "    \n",
    "    # Store metrics for this run\n",
    "    all_train_losses[num_comp_neurons] = epoch_train_losses\n",
    "    all_test_losses[num_comp_neurons] = epoch_test_losses\n",
    "    all_test_accuracies[num_comp_neurons] = epoch_test_accuracies\n",
    "    all_entropy_layer1[num_comp_neurons] = epoch_entropy_layer1\n",
    "    all_entropy_layer2[num_comp_neurons] = epoch_entropy_layer2\n",
    "    all_selection_ratios_layer1[num_comp_neurons] = epoch_selection_ratios_layer1\n",
    "    all_selection_ratios_layer2[num_comp_neurons] = epoch_selection_ratios_layer2\n",
    "\n",
    "# Plotting results\n",
    "epochs = range(1, num_epochs + 1)\n",
    "\n",
    "# Plot loss over epochs\n",
    "plt.figure(figsize=(10,6))\n",
    "for num_comp_neurons in comp_neurons_list:\n",
    "    plt.plot(epochs, all_train_losses[num_comp_neurons], label=f'Comp Neurons {num_comp_neurons}')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Train Loss')\n",
    "plt.title('Train Loss over Epochs')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# Plot test accuracy over epochs\n",
    "plt.figure(figsize=(10,6))\n",
    "for num_comp_neurons in comp_neurons_list:\n",
    "    plt.plot(epochs, all_test_accuracies[num_comp_neurons], label=f'Comp Neurons {num_comp_neurons}')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Test Accuracy (%)')\n",
    "plt.title('Test Accuracy over Epochs')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# Plot entropy over epochs for Layer 1\n",
    "plt.figure(figsize=(10,6))\n",
    "for num_comp_neurons in comp_neurons_list:\n",
    "    entropy_values = [np.sum(e) for e in all_entropy_layer1[num_comp_neurons]]\n",
    "    plt.plot(epochs, entropy_values, label=f'Comp Neurons {num_comp_neurons}')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Entropy (Layer 1)')\n",
    "plt.title('Entropy over Epochs (Layer 1)')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# Plot entropy over epochs for Layer 2\n",
    "plt.figure(figsize=(10,6))\n",
    "for num_comp_neurons in comp_neurons_list:\n",
    "    entropy_values = [np.sum(e) for e in all_entropy_layer2[num_comp_neurons]]\n",
    "    plt.plot(epochs, entropy_values, label=f'Comp Neurons {num_comp_neurons}')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Entropy (Layer 2)')\n",
    "plt.title('Entropy over Epochs (Layer 2)')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# Plot selection ratios over epochs for Layer 1\n",
    "plt.figure(figsize=(10,6))\n",
    "for num_comp_neurons in comp_neurons_list:\n",
    "    selection_ratios = np.array(all_selection_ratios_layer1[num_comp_neurons])  # Shape: [num_epochs, num_comp_neurons]\n",
    "    for i in range(num_comp_neurons):\n",
    "        plt.plot(epochs, selection_ratios[:, i], label=f'Comp Neurons {num_comp_neurons} - Neuron {i}')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Selection Ratio')\n",
    "plt.title('Selection Ratios over Epochs (Layer 1)')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# Plot selection ratios over epochs for Layer 2\n",
    "plt.figure(figsize=(10,6))\n",
    "for num_comp_neurons in comp_neurons_list:\n",
    "    selection_ratios = np.array(all_selection_ratios_layer2[num_comp_neurons])  # Shape: [num_epochs, num_comp_neurons]\n",
    "    for i in range(num_comp_neurons):\n",
    "        plt.plot(epochs, selection_ratios[:, i], label=f'Comp Neurons {num_comp_neurons} - Neuron {i}')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Selection Ratio')\n",
    "plt.title('Selection Ratios over Epochs (Layer 2)')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20dc2f3b-66e8-4d7d-b6ee-c4ce5a39e24c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
